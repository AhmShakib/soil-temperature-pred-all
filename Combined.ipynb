{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1b39e1-248b-4042-82b4-11cade2ae4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP Laptop 15\\tfenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>SoilTemperature-10</th>\n",
       "      <th>SoilTemperature-30</th>\n",
       "      <th>SoilTemperature-50</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Min Temperature</th>\n",
       "      <th>Max Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Surface Pressure</th>\n",
       "      <th>Solar Radiation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>17.40</td>\n",
       "      <td>18.81</td>\n",
       "      <td>19.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.38</td>\n",
       "      <td>13.04</td>\n",
       "      <td>25.44</td>\n",
       "      <td>62.68</td>\n",
       "      <td>7.94</td>\n",
       "      <td>2.17</td>\n",
       "      <td>101268.29</td>\n",
       "      <td>199.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>19.88</td>\n",
       "      <td>19.98</td>\n",
       "      <td>20.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.61</td>\n",
       "      <td>16.90</td>\n",
       "      <td>28.56</td>\n",
       "      <td>61.71</td>\n",
       "      <td>7.19</td>\n",
       "      <td>1.98</td>\n",
       "      <td>101039.08</td>\n",
       "      <td>210.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>24.08</td>\n",
       "      <td>23.32</td>\n",
       "      <td>23.17</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.62</td>\n",
       "      <td>20.73</td>\n",
       "      <td>31.84</td>\n",
       "      <td>55.48</td>\n",
       "      <td>8.75</td>\n",
       "      <td>2.10</td>\n",
       "      <td>100882.56</td>\n",
       "      <td>251.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>27.62</td>\n",
       "      <td>27.02</td>\n",
       "      <td>26.77</td>\n",
       "      <td>46.0</td>\n",
       "      <td>29.06</td>\n",
       "      <td>24.71</td>\n",
       "      <td>32.06</td>\n",
       "      <td>65.63</td>\n",
       "      <td>8.55</td>\n",
       "      <td>2.70</td>\n",
       "      <td>100655.40</td>\n",
       "      <td>257.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>28.54</td>\n",
       "      <td>28.29</td>\n",
       "      <td>28.21</td>\n",
       "      <td>402.0</td>\n",
       "      <td>27.69</td>\n",
       "      <td>24.18</td>\n",
       "      <td>32.09</td>\n",
       "      <td>77.39</td>\n",
       "      <td>6.10</td>\n",
       "      <td>2.37</td>\n",
       "      <td>100234.63</td>\n",
       "      <td>172.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>30.11</td>\n",
       "      <td>29.82</td>\n",
       "      <td>29.64</td>\n",
       "      <td>386.0</td>\n",
       "      <td>28.03</td>\n",
       "      <td>25.55</td>\n",
       "      <td>31.75</td>\n",
       "      <td>83.17</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.64</td>\n",
       "      <td>99934.18</td>\n",
       "      <td>140.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>7</td>\n",
       "      <td>30.74</td>\n",
       "      <td>30.53</td>\n",
       "      <td>30.64</td>\n",
       "      <td>202.0</td>\n",
       "      <td>28.83</td>\n",
       "      <td>25.92</td>\n",
       "      <td>31.60</td>\n",
       "      <td>80.26</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.87</td>\n",
       "      <td>99991.05</td>\n",
       "      <td>169.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>8</td>\n",
       "      <td>30.99</td>\n",
       "      <td>30.99</td>\n",
       "      <td>30.86</td>\n",
       "      <td>205.0</td>\n",
       "      <td>29.46</td>\n",
       "      <td>25.23</td>\n",
       "      <td>31.50</td>\n",
       "      <td>82.19</td>\n",
       "      <td>5.15</td>\n",
       "      <td>3.01</td>\n",
       "      <td>100129.68</td>\n",
       "      <td>181.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>29.83</td>\n",
       "      <td>29.89</td>\n",
       "      <td>29.95</td>\n",
       "      <td>209.0</td>\n",
       "      <td>28.71</td>\n",
       "      <td>24.34</td>\n",
       "      <td>32.42</td>\n",
       "      <td>82.53</td>\n",
       "      <td>4.65</td>\n",
       "      <td>2.14</td>\n",
       "      <td>100499.17</td>\n",
       "      <td>170.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>10</td>\n",
       "      <td>28.00</td>\n",
       "      <td>28.29</td>\n",
       "      <td>28.50</td>\n",
       "      <td>177.0</td>\n",
       "      <td>27.63</td>\n",
       "      <td>22.83</td>\n",
       "      <td>31.44</td>\n",
       "      <td>80.45</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.59</td>\n",
       "      <td>100864.38</td>\n",
       "      <td>167.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region  Year  Month  SoilTemperature-10  SoilTemperature-30  \\\n",
       "0       2  2001      1               17.40               18.81   \n",
       "1       2  2001      2               19.88               19.98   \n",
       "2       2  2001      3               24.08               23.32   \n",
       "3       2  2001      4               27.62               27.02   \n",
       "4       2  2001      5               28.54               28.29   \n",
       "5       2  2001      6               30.11               29.82   \n",
       "6       2  2001      7               30.74               30.53   \n",
       "7       2  2001      8               30.99               30.99   \n",
       "8       2  2001      9               29.83               29.89   \n",
       "9       2  2001     10               28.00               28.29   \n",
       "\n",
       "   SoilTemperature-50  Rainfall  Temperature  Min Temperature  \\\n",
       "0               19.43       0.0        18.38            13.04   \n",
       "1               20.36       1.0        22.61            16.90   \n",
       "2               23.17      33.0        26.62            20.73   \n",
       "3               26.77      46.0        29.06            24.71   \n",
       "4               28.21     402.0        27.69            24.18   \n",
       "5               29.64     386.0        28.03            25.55   \n",
       "6               30.64     202.0        28.83            25.92   \n",
       "7               30.86     205.0        29.46            25.23   \n",
       "8               29.95     209.0        28.71            24.34   \n",
       "9               28.50     177.0        27.63            22.83   \n",
       "\n",
       "   Max Temperature  Humidity  Sunshine  Wind Speed  Surface Pressure  \\\n",
       "0            25.44     62.68      7.94        2.17         101268.29   \n",
       "1            28.56     61.71      7.19        1.98         101039.08   \n",
       "2            31.84     55.48      8.75        2.10         100882.56   \n",
       "3            32.06     65.63      8.55        2.70         100655.40   \n",
       "4            32.09     77.39      6.10        2.37         100234.63   \n",
       "5            31.75     83.17      3.51        3.64          99934.18   \n",
       "6            31.60     80.26      4.55        3.87          99991.05   \n",
       "7            31.50     82.19      5.15        3.01         100129.68   \n",
       "8            32.42     82.53      4.65        2.14         100499.17   \n",
       "9            31.44     80.45      5.60        1.59         100864.38   \n",
       "\n",
       "   Solar Radiation  \n",
       "0           199.25  \n",
       "1           210.73  \n",
       "2           251.75  \n",
       "3           257.46  \n",
       "4           172.32  \n",
       "5           140.45  \n",
       "6           169.40  \n",
       "7           181.66  \n",
       "8           170.22  \n",
       "9           167.89  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, SimpleRNN, Dropout, Dense,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"ReadySoilDataset_Final.csv\")\n",
    "df = df[df['Region'].isin([2,4,5,7])].reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "473fcdc1-fd41-4473-bcc8-8b37c2c9163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['Month', 'Temperature', 'Min Temperature',\n",
    "               'Max Temperature', \n",
    "               'Rainfall', 'Surface Pressure']]\n",
    "target = df['SoilTemperature-50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4fd7c4-ebbe-44b3-8924-828ca9b8a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP Laptop 15\\tfenv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HP Laptop 15\\tfenv\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\HP Laptop 15\\tfenv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HP Laptop 15\\tfenv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "27/27 [==============================] - 7s 64ms/step - loss: 0.1106 - mae: 0.2704 - val_loss: 0.0403 - val_mae: 0.1717 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "11/27 [===========>..................] - ETA: 0s - loss: 0.0460 - mae: 0.1770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Laptop 15\\tfenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0398 - mae: 0.1630 - val_loss: 0.0171 - val_mae: 0.1097 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0228 - mae: 0.1184 - val_loss: 0.0124 - val_mae: 0.0863 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0194 - mae: 0.1090 - val_loss: 0.0119 - val_mae: 0.0830 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0219 - mae: 0.1145 - val_loss: 0.0089 - val_mae: 0.0707 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0182 - mae: 0.1059 - val_loss: 0.0088 - val_mae: 0.0690 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0156 - mae: 0.0975 - val_loss: 0.0091 - val_mae: 0.0745 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0160 - mae: 0.0960 - val_loss: 0.0085 - val_mae: 0.0697 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 0.0155 - mae: 0.0958 - val_loss: 0.0081 - val_mae: 0.0687 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0145 - mae: 0.0934 - val_loss: 0.0092 - val_mae: 0.0769 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0134 - mae: 0.0905 - val_loss: 0.0082 - val_mae: 0.0684 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0152 - mae: 0.0945 - val_loss: 0.0100 - val_mae: 0.0820 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0137 - mae: 0.0908 - val_loss: 0.0084 - val_mae: 0.0698 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0129 - mae: 0.0867 - val_loss: 0.0078 - val_mae: 0.0673 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0108 - mae: 0.0799 - val_loss: 0.0070 - val_mae: 0.0618 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0111 - mae: 0.0829 - val_loss: 0.0076 - val_mae: 0.0683 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0117 - mae: 0.0831 - val_loss: 0.0078 - val_mae: 0.0693 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0111 - mae: 0.0793 - val_loss: 0.0069 - val_mae: 0.0611 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 0.0107 - mae: 0.0794 - val_loss: 0.0067 - val_mae: 0.0604 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0110 - mae: 0.0805 - val_loss: 0.0074 - val_mae: 0.0669 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0100 - mae: 0.0761 - val_loss: 0.0067 - val_mae: 0.0626 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0098 - mae: 0.0757 - val_loss: 0.0067 - val_mae: 0.0624 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.0832 - val_loss: 0.0072 - val_mae: 0.0657 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0096 - mae: 0.0752 - val_loss: 0.0071 - val_mae: 0.0654 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0082 - mae: 0.0695 - val_loss: 0.0069 - val_mae: 0.0636 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0089 - mae: 0.0712 - val_loss: 0.0067 - val_mae: 0.0609 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0087 - mae: 0.0691 - val_loss: 0.0072 - val_mae: 0.0651 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0088 - mae: 0.0712 - val_loss: 0.0071 - val_mae: 0.0646 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0085 - mae: 0.0692 - val_loss: 0.0071 - val_mae: 0.0650 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0090 - mae: 0.0715 - val_loss: 0.0070 - val_mae: 0.0643 - lr: 4.0000e-05\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0084 - mae: 0.0689 - val_loss: 0.0069 - val_mae: 0.0630 - lr: 4.0000e-05\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0087 - mae: 0.0705 - val_loss: 0.0068 - val_mae: 0.0627 - lr: 4.0000e-05\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0084 - mae: 0.0695 - val_loss: 0.0069 - val_mae: 0.0631 - lr: 4.0000e-05\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0090 - mae: 0.0719 - val_loss: 0.0068 - val_mae: 0.0627 - lr: 4.0000e-05\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0078 - mae: 0.0681 - val_loss: 0.0068 - val_mae: 0.0621 - lr: 8.0000e-06\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 0s 11ms/step - loss: 0.0083 - mae: 0.0696 - val_loss: 0.0068 - val_mae: 0.0622 - lr: 8.0000e-06\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0057 - mae: 0.0577\n",
      "Test Loss (MSE): 0.0057\n",
      "Test MAE:       0.0577\n",
      "Epoch 1/100\n",
      "27/27 [==============================] - 3s 22ms/step - loss: 0.3510 - mae: 0.4661 - val_loss: 0.0480 - val_mae: 0.1667 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1821 - mae: 0.3443 - val_loss: 0.0435 - val_mae: 0.1719 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1545 - mae: 0.3128 - val_loss: 0.0284 - val_mae: 0.1375 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1109 - mae: 0.2639 - val_loss: 0.0186 - val_mae: 0.1004 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0742 - mae: 0.2169 - val_loss: 0.0124 - val_mae: 0.0839 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0692 - mae: 0.2069 - val_loss: 0.0115 - val_mae: 0.0824 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0518 - mae: 0.1796 - val_loss: 0.0105 - val_mae: 0.0806 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0452 - mae: 0.1653 - val_loss: 0.0124 - val_mae: 0.0879 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0411 - mae: 0.1606 - val_loss: 0.0128 - val_mae: 0.0890 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0394 - mae: 0.1581 - val_loss: 0.0114 - val_mae: 0.0847 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0361 - mae: 0.1477 - val_loss: 0.0114 - val_mae: 0.0830 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0337 - mae: 0.1465 - val_loss: 0.0093 - val_mae: 0.0761 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0326 - mae: 0.1413 - val_loss: 0.0130 - val_mae: 0.0935 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0305 - mae: 0.1368 - val_loss: 0.0176 - val_mae: 0.1111 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0282 - mae: 0.1336 - val_loss: 0.0197 - val_mae: 0.1180 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0255 - mae: 0.1253 - val_loss: 0.0169 - val_mae: 0.1097 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0285 - mae: 0.1297 - val_loss: 0.0130 - val_mae: 0.0943 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0243 - mae: 0.1227 - val_loss: 0.0146 - val_mae: 0.1006 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0226 - mae: 0.1186 - val_loss: 0.0136 - val_mae: 0.0962 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0247 - mae: 0.1223 - val_loss: 0.0150 - val_mae: 0.1015 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0231 - mae: 0.1201 - val_loss: 0.0159 - val_mae: 0.1053 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0223 - mae: 0.1151 - val_loss: 0.0137 - val_mae: 0.0967 - lr: 2.0000e-04\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0772\n",
      "Test Loss (MSE): 0.0093\n",
      "Test MAE:       0.0772\n",
      "Epoch 1/100\n",
      "27/27 [==============================] - 5s 47ms/step - loss: 0.1356 - mae: 0.2902 - val_loss: 0.0323 - val_mae: 0.1529 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0305 - mae: 0.1400 - val_loss: 0.0144 - val_mae: 0.0895 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0222 - mae: 0.1168 - val_loss: 0.0104 - val_mae: 0.0748 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0187 - mae: 0.1048 - val_loss: 0.0085 - val_mae: 0.0694 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0169 - mae: 0.0985 - val_loss: 0.0081 - val_mae: 0.0694 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0150 - mae: 0.0946 - val_loss: 0.0094 - val_mae: 0.0764 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0145 - mae: 0.0927 - val_loss: 0.0075 - val_mae: 0.0649 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0139 - mae: 0.0913 - val_loss: 0.0071 - val_mae: 0.0644 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0117 - mae: 0.0841 - val_loss: 0.0078 - val_mae: 0.0648 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.0867 - val_loss: 0.0070 - val_mae: 0.0644 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0121 - mae: 0.0826 - val_loss: 0.0069 - val_mae: 0.0617 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.0820 - val_loss: 0.0067 - val_mae: 0.0618 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0119 - mae: 0.0817 - val_loss: 0.0071 - val_mae: 0.0645 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0111 - mae: 0.0818 - val_loss: 0.0070 - val_mae: 0.0643 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.0813 - val_loss: 0.0071 - val_mae: 0.0642 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0105 - mae: 0.0792 - val_loss: 0.0071 - val_mae: 0.0620 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0109 - mae: 0.0783 - val_loss: 0.0078 - val_mae: 0.0703 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 15ms/step - loss: 0.0098 - mae: 0.0745 - val_loss: 0.0064 - val_mae: 0.0601 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0097 - mae: 0.0754 - val_loss: 0.0068 - val_mae: 0.0638 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0095 - mae: 0.0741 - val_loss: 0.0064 - val_mae: 0.0614 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0093 - mae: 0.0721 - val_loss: 0.0067 - val_mae: 0.0618 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0101 - mae: 0.0764 - val_loss: 0.0065 - val_mae: 0.0617 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0098 - mae: 0.0750 - val_loss: 0.0068 - val_mae: 0.0628 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0091 - mae: 0.0704 - val_loss: 0.0066 - val_mae: 0.0613 - lr: 4.0000e-05\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0098 - mae: 0.0737 - val_loss: 0.0066 - val_mae: 0.0616 - lr: 4.0000e-05\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0092 - mae: 0.0725 - val_loss: 0.0066 - val_mae: 0.0615 - lr: 4.0000e-05\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.0087 - mae: 0.0706 - val_loss: 0.0067 - val_mae: 0.0625 - lr: 4.0000e-05\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0089 - mae: 0.0719 - val_loss: 0.0066 - val_mae: 0.0618 - lr: 4.0000e-05\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0053 - mae: 0.0559\n",
      "Test Loss (MSE): 0.0053\n",
      "Test MAE:       0.0559\n",
      "Epoch 1/100\n",
      "27/27 [==============================] - 5s 46ms/step - loss: 0.1090 - mae: 0.2568 - val_loss: 0.0114 - val_mae: 0.0815 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0292 - mae: 0.1334 - val_loss: 0.0093 - val_mae: 0.0708 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0249 - mae: 0.1230 - val_loss: 0.0078 - val_mae: 0.0667 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0214 - mae: 0.1128 - val_loss: 0.0077 - val_mae: 0.0660 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0191 - mae: 0.1066 - val_loss: 0.0090 - val_mae: 0.0752 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0182 - mae: 0.1025 - val_loss: 0.0075 - val_mae: 0.0671 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0159 - mae: 0.0977 - val_loss: 0.0078 - val_mae: 0.0683 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0160 - mae: 0.0956 - val_loss: 0.0071 - val_mae: 0.0618 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0139 - mae: 0.0905 - val_loss: 0.0069 - val_mae: 0.0637 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0147 - mae: 0.0931 - val_loss: 0.0076 - val_mae: 0.0675 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0139 - mae: 0.0908 - val_loss: 0.0076 - val_mae: 0.0672 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0144 - mae: 0.0928 - val_loss: 0.0069 - val_mae: 0.0613 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0136 - mae: 0.0885 - val_loss: 0.0064 - val_mae: 0.0603 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0116 - mae: 0.0839 - val_loss: 0.0069 - val_mae: 0.0614 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0120 - mae: 0.0824 - val_loss: 0.0074 - val_mae: 0.0660 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0106 - mae: 0.0793 - val_loss: 0.0086 - val_mae: 0.0733 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0123 - mae: 0.0848 - val_loss: 0.0070 - val_mae: 0.0634 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0104 - mae: 0.0786 - val_loss: 0.0065 - val_mae: 0.0601 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0104 - mae: 0.0771 - val_loss: 0.0069 - val_mae: 0.0635 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0114 - mae: 0.0805 - val_loss: 0.0067 - val_mae: 0.0630 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0108 - mae: 0.0789 - val_loss: 0.0069 - val_mae: 0.0636 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 0s 13ms/step - loss: 0.0115 - mae: 0.0826 - val_loss: 0.0069 - val_mae: 0.0633 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.0104 - mae: 0.0762 - val_loss: 0.0069 - val_mae: 0.0631 - lr: 2.0000e-04\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.0057 - mae: 0.0591\n",
      "Test Loss (MSE): 0.0057\n",
      "Test MAE:       0.0591\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feature scaling with StandardScaler\n",
    "feat_scaler = StandardScaler()\n",
    "X_scaled = feat_scaler.fit_transform(features)\n",
    "\n",
    "# Target scaling with MinMaxScaler\n",
    "tgt_scaler = MinMaxScaler()\n",
    "y_scaled = tgt_scaler.fit_transform(target.values.reshape(-1, 1))\n",
    "# === 4. Sliding-window sequencing ============================================\n",
    "def create_sliding_window(X, y, window_size):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - window_size):\n",
    "        Xs.append(X[i:i + window_size])\n",
    "        ys.append(y[i + window_size])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "WINDOW = 12  # Define the window size for time-series sequences\n",
    "X_seq, y_seq = create_sliding_window(X_scaled, y_scaled, WINDOW)\n",
    "\n",
    "# === 5. Train / val / test split =============================================\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X_seq, y_seq,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp,\n",
    "                                                test_size=0.5,\n",
    "                                                random_state=2)\n",
    "\n",
    "\n",
    "cnn_lstm_model = Sequential([\n",
    "    # Conv1D Layer for feature extraction\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    # LSTM Layers\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    # Fully connected layers\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "cnn_lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# 2. SimpleRNN Model Definition\n",
    "simple_rnn_model = Sequential([\n",
    "    SimpleRNN(64, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    SimpleRNN(32, activation='tanh', return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "simple_rnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# 3. LSTM Model Definition\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# 4. GRU Model Definition\n",
    "gru_model = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    GRU(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "gru_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# === 5. Callbacks ============================================================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "lr_sched = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# === 6. Train the models =====================================================\n",
    "# Train CNN-LSTM\n",
    "cnn_lstm_history = cnn_lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, checkpoint, lr_sched],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_lstm_loss, cnn_lstm_mae = cnn_lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss (MSE): {cnn_lstm_loss:.4f}\")\n",
    "print(f\"Test MAE:       {cnn_lstm_mae:.4f}\")\n",
    "\n",
    "# Train SimpleRNN\n",
    "simple_rnn_history = simple_rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, checkpoint, lr_sched],\n",
    "    verbose=1\n",
    ")\n",
    "simple_rnn_loss, simple_rnn_mae = simple_rnn_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss (MSE): {simple_rnn_loss:.4f}\")\n",
    "print(f\"Test MAE:       {simple_rnn_mae:.4f}\")\n",
    "\n",
    "# Train LSTM\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, checkpoint, lr_sched],\n",
    "    verbose=1\n",
    ")\n",
    "lstm_loss, lstm_mae = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss (MSE): {lstm_loss:.4f}\")\n",
    "print(f\"Test MAE:       {lstm_mae:.4f}\")\n",
    "\n",
    "# Train GRU\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, checkpoint, lr_sched],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gru_loss, gru_mae = gru_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss (MSE): {gru_loss:.4f}\")\n",
    "print(f\"Test MAE:       {gru_mae:.4f}\")\n",
    "# === 8. Plot the comparison ===================================================\n",
    "# Create list of model names and their test losses\n",
    "models = ['CNN-LSTM', 'SimpleRNN', 'LSTM', 'GRU']\n",
    "test_losses = [cnn_lstm_loss, simple_rnn_loss, lstm_loss, gru_loss]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140432fe-cc32-4baa-bf59-36bd6fc3464c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, test_losses, color=['blue', 'green', 'red','pink'])\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b01b9a5-231a-47ca-9342-079bd44d7ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005388159770518541, 0.011869343928992748, 0.00530689861625433, 0.005558584816753864]\n"
     ]
    }
   ],
   "source": [
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39d0e0-82d1-4d3a-a319-6b057d10f2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
